<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml"/><link href="/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-05T06:49:44+00:00</updated><id>/feed.xml</id><title type="html">Alexander Aghili</title><subtitle></subtitle><entry><title type="html">The Bio-Science AI Hackathon</title><link href="/blog/2024/QBI-Hackthon/" rel="alternate" type="text/html" title="The Bio-Science AI Hackathon"/><published>2024-08-06T00:00:00+00:00</published><updated>2024-08-06T00:00:00+00:00</updated><id>/blog/2024/QBI-Hackthon</id><content type="html" xml:base="/blog/2024/QBI-Hackthon/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, I had the chance to join the QBI bio-hackathon at UCSF. This year’s focus was on using machine learning and artificial intelligence to advance medicine. There were many incredible projects, like breast cancer detection using imaging and protein visualization. I was excited about contributing my computing skills to medical problems. Technology advancements that improve medicine can significantly enhance people’s lives.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Hackathon-480.webp 480w,/assets/img/QBIHackathon/Hackathon-800.webp 800w,/assets/img/QBIHackathon/Hackathon-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Hackathon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Initially, I didn’t have a team. Fortunately, we were given the opportunity to pitch ideas and join existing teams. One pitch that caught my attention was from a Ph.D. researcher at UCSF, who was grappling with a challenging issue related to scientific data.</p> <p>Advancements in Artificial Intelligence and Machine Learning in medicine depend on high-quality data for successful results. For example, Alpha Fold, arguably the most successful AI tool in biology to date, relied on the Protein Data Bank, a manually curated database of biomolecular structures for training. This data bank was assembled by researchers throughout the field collaborating and contributing in this centralized repository. Thus, for further predictive analysis capabilites, we need more data.</p> <p>However, getting the correct data and metadata is difficult, as researchers do not like spending time on data formatting and deposition. Oftentimes, they may not be aware or simply forget to input the data into the data bank whenever they have discovered a new structure. Thus, to solve this issue, we want to scan through recently published papers that report structures of biomolecules so we can reach out to authors and bring their data into PDB.</p> <p>The researcher leading this hackathon project, over the previous summer, had interns manually go through papers and fill out a spreadsheet with information about the article. Such information includes which methods were used to image the protein or if multiple methods were used. The team could then reach out to the researchers and encourage/collaborate with them to input their data into the protein data bank. The given task, for our hackathon, was to try and automate this pipeline. With recent advancements using LLMs, we figured it’d be possible to do the entire pipeline.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Data-Interns-480.webp 480w,/assets/img/QBIHackathon/Data-Interns-800.webp 800w,/assets/img/QBIHackathon/Data-Interns-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Data-Interns.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="project-description">Project Description</h2> <p>Here were the specific goals we were tasked with:</p> <ol> <li>A Python script that accepts a paper (plain text or pdf) and performs annotation with a set of predefined questions.</li> <li>A Python script, that pulls updates from a set of bio/medRxiv subtopics, calls script 1, puts results into a GoogleSheet and triggers email notification.</li> <li>Purely open source/free software implementation using open/free LLMs or other approaches.</li> </ol> <p>Since the relevant papers are available through bioRxiv and medRxiv, we used those as the data source. They conveniently have an API to access journal data as well.</p> <p>To get the prototype working, we decided to use the GPT 4.0 API. While this does break the 3rd goal, the LLM is easily replaceable by an open source alternative. The basic pipeline we created is as follows. We get the papers from the bioRxiv API and extract the metadata from the content. We turn the content into vector embeddings using OpenAI’s embeddings and store the embeddings in the Lantern Vector+postgres database, who was sponsoring the event. This helped us store the data about the papers for the long term and ask additional questions if necessary. We then used FAISS context retrieval to retrieve the context based on a given input using cosine similarity. We use GPT 4.0 to annotate the retrieved data and use Google Sheets and Mail APIs to save the data and notify stakeholders. We use AWS to host the architecture through an EC2 instance. We used langchain to help with LLM utilities.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/AWS-pipeline-480.webp 480w,/assets/img/QBIHackathon/AWS-pipeline-800.webp 800w,/assets/img/QBIHackathon/AWS-pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/AWS-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="implementation">Implementation</h2> <p>I will now go into depth for each part of the architecture with code. Starting with the bioRxiv scraper, in scraper.py: We utilized a python library that provided the <a href="https://api.biorxiv.org/">bioRxiv API</a> functionality for us called <a href="https://pypi.org/project/paperscraper/">paperscraper</a>.</p> <p>This allowed us to use the function bioXriv to download pdf versions of papers published between a certain date.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Scrapes papers from bioRxiv between the specified dates and saves the metadata in a JSON file.

:param start: Start date for the scraping (format: </span><span class="sh">"</span><span class="s">YYYY-MM-DD</span><span class="sh">"</span><span class="s">).
:param end: End date for the scraping (format: </span><span class="sh">"</span><span class="s">YYYY-MM-DD</span><span class="sh">"</span><span class="s">).
:param out_file: Output file to save the metadata in JSON Lines format.
:return: None
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">scrapeBiorxiv</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">out_file</span><span class="p">):</span>
    <span class="n">filepath</span> <span class="o">=</span> <span class="n">out_file</span>
    <span class="nf">biorxiv</span><span class="p">(</span><span class="n">begin_date</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">end_date</span><span class="o">=</span><span class="n">end</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="n">out_file</span><span class="p">)</span>
    <span class="nf">retreiveTextFromPdf</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>

</code></pre></div></div> <p>After downloading all the papers, we scrape them using the retreiveTextFromPdf function. For each unique article (identified by DOI stored in the SQL database), we extract the text from the pdf, and get vector embeddings using the get_embeddings() function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Retrieves text from PDF files, extracts embeddings, and stores information in a custom database.

:param inp_file: Path to the input JSON file containing paper metadata.
:return: None
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">retreiveTextFromPdf</span><span class="p">(</span><span class="n">inp_file</span><span class="p">):</span>

    <span class="n">json</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_json</span><span class="p">(</span><span class="n">path_or_buf</span><span class="o">=</span><span class="n">inp_file</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">lantern</span> <span class="o">=</span> <span class="nc">Lantern</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">doi</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">json</span><span class="p">[</span><span class="sh">'</span><span class="s">doi</span><span class="sh">'</span><span class="p">]):</span>

        <span class="n">paper_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">doi</span><span class="sh">'</span><span class="p">:</span> <span class="n">doi</span><span class="p">}</span>
        <span class="n">doi</span> <span class="o">=</span> <span class="n">doi</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">lantern</span><span class="p">.</span><span class="nf">publicationExists</span><span class="p">(</span><span class="n">doi</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="n">pdf_dir</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./papers/</span><span class="sh">'</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">pdf_dir</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">mkdir</span><span class="p">(</span><span class="n">pdf_dir</span><span class="p">)</span>

        <span class="n">pdfsavefile</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./papers/</span><span class="sh">'</span> <span class="o">+</span> <span class="n">doi</span> <span class="o">+</span> <span class="sh">'</span><span class="s">.pdf</span><span class="sh">'</span>
        <span class="nf">save_pdf</span><span class="p">(</span><span class="n">paper_data</span><span class="p">,</span> <span class="n">filepath</span><span class="o">=</span><span class="n">pdfsavefile</span><span class="p">)</span>

        <span class="c1"># creating a pdf reader object
</span>        <span class="n">reader</span> <span class="o">=</span> <span class="n">PyPDF2</span><span class="p">.</span><span class="nc">PdfReader</span><span class="p">(</span><span class="n">pdfsavefile</span><span class="p">)</span>
        <span class="n">save_txt_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">scrapped_txts/</span><span class="sh">'</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">save_txt_path</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">mkdir</span><span class="p">(</span><span class="n">save_txt_path</span><span class="p">)</span>
        <span class="n">extract_text</span> <span class="o">=</span> <span class="sh">''</span>
        <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">.</span><span class="n">pages</span><span class="p">:</span>
            <span class="n">extract_text</span> <span class="o">+=</span> <span class="n">page</span><span class="p">.</span><span class="nf">extract_text</span><span class="p">()</span>

        <span class="n">txt_file</span> <span class="o">=</span> <span class="nf">str</span><span class="p">(</span><span class="sh">'</span><span class="s">{}.txt</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">doi</span><span class="p">))</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">save_txt_path</span> <span class="o">+</span> <span class="n">txt_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
            <span class="nb">file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">extract_text</span><span class="p">)</span>

        <span class="n">txt_embs</span><span class="p">,</span> <span class="n">emb</span> <span class="o">=</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">save_txt_path</span> <span class="o">+</span> <span class="n">txt_file</span><span class="p">)</span>
        <span class="bp">...</span>
</code></pre></div></div> <p>Get_embeddings will split the input text into chunks and create a vector embedding using OpenAI Embeddings for each chunk.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="sh">"""</span><span class="s">
Retrieves text embeddings from a given text file using OpenAI</span><span class="sh">'</span><span class="s">s language model.

:param fname: Path to the input text file.
:return: A tuple containing text embeddings and the OpenAIEmbeddings instance.
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="nc">TextLoader</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">CharacterTextSplitter</span><span class="p">(</span>
        <span class="n">separator</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="n">emb</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>
    <span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">]</span>

    <span class="n">input_embeddings</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">embed_documents</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>
    <span class="n">text_embeddings</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">input_texts</span><span class="p">,</span> <span class="n">input_embeddings</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">text_embeddings</span><span class="p">,</span> <span class="n">emb</span>

</code></pre></div></div> <p>To get an understanding of the next part, we have to understand the database structure. We decided to have to main tables with the following structures:</p> <ol> <li>The Publication Table - Holds publication metadata <ol> <li>DOI (Digital Object Identifier) - Universal Paper Identifier</li> <li>The title</li> <li>pmc link</li> <li>pubmed link</li> </ol> </li> <li>The Fragments Table <ol> <li>DOI (Foreign Key used to identify which paper fragment is from)</li> <li>Header</li> <li>Content</li> <li>Vector</li> </ol> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Class to represent a publication with attributes id, title, pmc, pubmed, and doi
</span><span class="k">class</span> <span class="nc">Publication</span><span class="p">:</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">title</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">pmc</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">pubmed</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">doi</span> <span class="o">=</span> <span class="sh">""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">pmc</span><span class="p">,</span> <span class="n">pubmed</span><span class="p">,</span> <span class="n">doi</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">id</span>  <span class="c1"># (DOI) Unique identifier for the publication
</span>        <span class="n">self</span><span class="p">.</span><span class="n">title</span> <span class="o">=</span> <span class="n">title</span>  <span class="c1"># Title of the publication
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pmc</span> <span class="o">=</span> <span class="n">pmc</span>  <span class="c1"># PubMed Central (PMC) Link
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pubmed</span> <span class="o">=</span> <span class="n">pubmed</span>  <span class="c1"># PubMed Link
</span>        <span class="n">self</span><span class="p">.</span><span class="n">doi</span> <span class="o">=</span> <span class="n">doi</span>  <span class="c1"># Digital Object Identifier (DOI) Link for the publication
</span>

<span class="c1"># Class to represent a fragment of a publication with attributes id, header, content, and vector
</span><span class="k">class</span> <span class="nc">Fragment</span><span class="p">:</span>
    <span class="c1"># Class variables to store default values for attributes
</span>    <span class="nb">id</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">header</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">content</span> <span class="o">=</span> <span class="sh">""</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="sh">""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">header</span><span class="p">,</span> <span class="n">content</span><span class="p">,</span> <span class="n">vector</span><span class="p">):</span>
        <span class="c1"># Constructor to initialize the attributes of the Fragment object
</span>
        <span class="c1"># Set the attributes of the object with the values provided during instantiation
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">id</span>  <span class="c1"># (DOI) Unique identifier for the fragment
</span>        <span class="n">self</span><span class="p">.</span><span class="n">header</span> <span class="o">=</span> <span class="n">header</span>  <span class="c1"># Header or title of the fragment
</span>        <span class="n">self</span><span class="p">.</span><span class="n">content</span> <span class="o">=</span> <span class="n">content</span>  <span class="c1"># Content or text of the fragment
</span>        <span class="n">self</span><span class="p">.</span><span class="n">vector</span> <span class="o">=</span> <span class="n">vector</span>  <span class="c1"># Vector representation of the fragment
</span>
</code></pre></div></div> <p>After we have all the embeddings, we create a list of Fragments and a Publication which we then store in the database using insertEmbeddings and insertPublication. These are simple wrappers on insert commands into the database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">retreiveTextFromPdf</span><span class="p">(</span><span class="n">inp_file</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="n">fragments</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">txt</span><span class="p">,</span> <span class="n">embs</span> <span class="ow">in</span> <span class="n">txt_embs</span><span class="p">:</span>
        <span class="n">fragment</span> <span class="o">=</span> <span class="nc">Fragment</span><span class="p">(</span><span class="n">doi</span><span class="p">,</span> <span class="sh">'</span><span class="s">methods</span><span class="sh">'</span><span class="p">,</span> <span class="n">txt</span><span class="p">,</span> <span class="n">embs</span><span class="p">)</span>
        <span class="n">fragments</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">fragment</span><span class="p">)</span>

    <span class="n">publication</span> <span class="o">=</span> <span class="nc">Publication</span><span class="p">(</span><span class="n">doi</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">pmc</span><span class="p">,</span> <span class="n">pubmed</span><span class="p">,</span> <span class="n">doi</span><span class="p">)</span>

    <span class="n">lantern</span><span class="p">.</span><span class="nf">insertEmbeddings</span><span class="p">(</span><span class="n">fragments</span><span class="p">)</span>
    <span class="n">lantern</span><span class="p">.</span><span class="nf">insertPublication</span><span class="p">(</span><span class="n">publication</span><span class="p">)</span>

    <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">pdfsavefile</span><span class="p">)</span>
</code></pre></div></div> <p>This code has the ability to be run automatically once a day through a script or with a fixed timespan if run manually.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Adding command line arguments for start_date and end_date with default values as the current date
</span>    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="nc">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Scrape and process scientific papers from bioRxiv.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--start-date</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="nf">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">date</span><span class="p">.</span><span class="nf">today</span><span class="p">()),</span> <span class="n">help</span><span class="o">=</span><span class="sh">"</span><span class="s">Start date for the scraping (format: </span><span class="sh">'</span><span class="s">YYYY-MM-DD</span><span class="sh">'</span><span class="s">).</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--end-date</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="nf">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">date</span><span class="p">.</span><span class="nf">today</span><span class="p">()),</span> <span class="n">help</span><span class="o">=</span><span class="sh">"</span><span class="s">End date for the scraping (format: </span><span class="sh">'</span><span class="s">YYYY-MM-DD</span><span class="sh">'</span><span class="s">).</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--outfile</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">bio.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="sh">"</span><span class="s">Output file to save the metadata in JSON Lines format.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>

    <span class="c1"># Calling the scrapeBiorxiv function with command line arguments
</span>    <span class="nf">scrapeBiorxiv</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">start_date</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">end_date</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">out_file</span><span class="p">)</span>
</code></pre></div></div> <p>The next part of the program is the document analyzer, which automates the evaluation of publications. This component of the system is designed to determine whether a paper mentions specific structural biology methods and subsequently updates the tracking systems with the results. The document analyzer is encapsulated in the DocumentAnalyzer class. It integrates with several services, including the Lantern vector database, Google Sheets for result tracking, and our custom LlmHandler class for handling interactions with the large language model (LLM).</p> <p>For every unread publication in the database, the text embeddings are retrieved to check if the paper is about cryo-EM, analyze the publication if it is, then update the spreadsheet with the results, and send an email notification if configured to do so.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"""</span><span class="s">
    pulls all new files from Lantern database, evaluates them, and publishes results to google sheets
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">analyze_all_unread</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">publications</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">lantern</span><span class="p">.</span><span class="nf">getUnreadPublications</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">process_publications</span><span class="p">(</span><span class="n">publications</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_publications</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">publications</span><span class="p">:</span> <span class="p">[</span><span class="n">Publication</span><span class="p">]):</span>
        <span class="sh">"""</span><span class="s">takes a list of publications, applies retrievalQA and processes responses

        Args:
            publications ([]): list of publications 
        </span><span class="sh">"""</span>

        <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">hits</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">pub</span> <span class="ow">in</span> <span class="n">publications</span><span class="p">:</span>
            <span class="n">text_embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">lantern</span><span class="p">.</span><span class="nf">get_embeddings_for_pub</span><span class="p">(</span><span class="n">pub</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
            <span class="n">classification</span><span class="p">,</span> <span class="n">response</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">''</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">paper_about_cryoem</span><span class="p">(</span><span class="n">text_embeddings</span><span class="p">):</span>
                <span class="n">classification</span><span class="p">,</span> <span class="n">response</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">analyze_publication</span><span class="p">(</span><span class="n">text_embeddings</span><span class="p">)</span>
                <span class="n">hits</span> <span class="o">+=</span> <span class="n">classification</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># print('paper not about cryo-em')
</span>                <span class="k">pass</span>
            <span class="c1"># add date if it's added 
</span>            <span class="n">rows</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">pub</span><span class="p">.</span><span class="n">doi</span><span class="p">,</span> <span class="n">pub</span><span class="p">.</span><span class="n">title</span><span class="p">,</span> <span class="sh">""</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">date</span><span class="p">.</span><span class="nf">today</span><span class="p">()),</span> <span class="sh">""</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">classification</span><span class="p">),</span> <span class="n">response</span><span class="p">,</span> <span class="sh">""</span><span class="p">])</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">update_spreadsheet</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">hits</span><span class="p">)</span>
</code></pre></div></div> <p>We know its unread since we used a separate table that only contains the id of publications that were inserted after the last read.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"""</span><span class="s">
    Retrieves unread publications from the </span><span class="sh">'</span><span class="s">publications</span><span class="sh">'</span><span class="s"> table.
    Parameters:
        - delete_unread_entries: bool, decides if entries are deleted from the </span><span class="sh">"</span><span class="s">unread</span><span class="sh">"</span><span class="s"> table
    Returns:
        - List[Publication], a list of Publication objects representing the unread publications.
    Notes:
        - Performs a left join between </span><span class="sh">'</span><span class="s">publications</span><span class="sh">'</span><span class="s"> and </span><span class="sh">'</span><span class="s">unread</span><span class="sh">'</span><span class="s"> tables to retrieve unread publications.
        - Clears the </span><span class="sh">'</span><span class="s">unread</span><span class="sh">'</span><span class="s"> table after retrieving the unread publications.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">getUnreadPublications</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">delete_unread_entries</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">conn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">conn</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>

        <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span>
            <span class="sh">'</span><span class="s">SELECT * FROM publications AS p LEFT JOIN unread AS u ON u.id=p.id;</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">publications</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">.</span><span class="nf">fetchall</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">delete_unread_entries</span><span class="p">:</span>
            <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">'</span><span class="s">DELETE FROM unread;</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
        <span class="n">cursor</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

        <span class="n">publicationObjects</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">publications</span><span class="p">:</span>
            <span class="n">publicationObjects</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="nc">Publication</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">publicationObjects</span>
</code></pre></div></div> <p>However, in retrospect, it might make more sense to just have an additional column in the publications table as to whether its unread.</p> <p>The analyze_publications function analyzes a publication to determine if it mentions any structural biology methods. It uses FAISS for embedding-based retrieval and the LLM for query evaluation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">analyze_publication</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text_embeddings</span><span class="p">:</span> <span class="p">[]):</span>
        <span class="sh">"""</span><span class="s">poses a question about the document, processes the result and returns it
        NOTE: for now, only uses the hackathon question, might add more later

        Args:
            text_embeddings ([]): list of (embedding, text) pairs from document to be analyzed
        
        Returns:
            bool: classification of response to query as positive (True) or negative (False) 
            str: response from chatGPT
        </span><span class="sh">"""</span>
        <span class="c1"># NOTE: These very likely need to change
</span>        <span class="n">open_ai_emb</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>
        <span class="n">query</span> <span class="o">=</span> <span class="nf">get_qbi_hackathon_prompt</span><span class="p">(</span><span class="n">METHODS_KEYWORDS</span><span class="p">)</span>
        <span class="n">faiss_index</span> <span class="o">=</span> <span class="n">FAISS</span><span class="p">.</span><span class="nf">from_embeddings</span><span class="p">(</span><span class="n">text_embeddings</span><span class="o">=</span><span class="n">text_embeddings</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">open_ai_emb</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">.</span><span class="nf">evaluate_queries</span><span class="p">(</span><span class="n">faiss_index</span><span class="p">,</span> <span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">classify_response</span><span class="p">(</span><span class="n">response</span><span class="p">),</span> <span class="n">response</span>
</code></pre></div></div> <p>We utilized several keywords to assist the LLM in identifing certain methodologies. We used the get_qbi_hackathon_prompt function to format our prompt using the keywords. They can be seen below as a guide:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># A list of abbreviated names and synonyms
# for various biophysical methonds
# that are typically used for integrative modeling
</span>
<span class="n">METHODS_KEYWORDS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">CX-MS</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">cross-link</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">crosslink</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">XL-MS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CX-MS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CL-MS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">XLMS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CXMS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CLMS</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">chemical crosslinking mass spectrometry</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">photo-crosslinking</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">crosslinking restraints</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">crosslinking-derived restraints</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chemical crosslinking</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">in vivo crosslinking</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">crosslinking data</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">HDX</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Hydrogen–deuterium exchange mass spectrometry</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Hydrogen/deuterium exchange mass spectrometry</span><span class="sh">'</span>
        <span class="sh">'</span><span class="s">HDX</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">HDXMS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">HDX-MS</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">EPR</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">electron paramagnetic resonance spectroscopy</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">EPR</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DEER</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Double electron electron resonance spectroscopy</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">FRET</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">FRET</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">forster resonance energy transfer</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fluorescence resonance energy transfer</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">AFM</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">AFM</span><span class="sh">'</span><span class="p">,</span>  <span class="sh">"</span><span class="s">atomic force microscopy</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">SAS</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">SAS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SAXS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SANS</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">Small angle solution scattering</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">solution scattering</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SEC-SAXS</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SEC-SAS</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SASBDB</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Small angle X-ray scattering</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Small angle neutron scattering</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">3DGENOME</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">HiC</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Hi-C</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">chromosome conformation capture</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">Y2H</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Y2H</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">yeast two-hybrid</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">DNA_FOOTPRINTING</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">DNA Footprinting</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">hydroxyl radical footprinting</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">XRAY_TOMOGRAPHY</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">soft x-ray tomography</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">FTIR</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">FTIR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Infrared spectroscopy</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Fourier-transform infrared spectroscopy</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">FLUORESCENCE</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">Fluorescence imaging</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fluorescence microscopy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">TIRF</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">EVOLUTION</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">coevolution</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">evolutionary covariance</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">PREDICTED</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">predicted contacts</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">INTEGRATIVE</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">integrative structure</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hybrid structure</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">integrative modeling</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hybrid modeling</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>

    <span class="sh">'</span><span class="s">SHAPE</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Hydroxyl Acylation analyzed by Primer Extension</span><span class="sh">'</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">get_qbi_hackathon_prompt</span><span class="p">(</span><span class="n">keywords</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Returns a prompt that was initially developed
    during the QBI Hackathon.
    </span><span class="sh">"""</span>

    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">raise</span><span class="p">(</span><span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Keywords dict can</span><span class="sh">'</span><span class="s">t be empty</span><span class="sh">"</span><span class="p">))</span>

    <span class="n">methods_string</span> <span class="o">=</span> <span class="nf">keywords_dict_to_string</span><span class="p">(</span><span class="n">keywords</span><span class="p">)</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sh">"</span><span class="s">You are reading a materials and methods section </span><span class="sh">"</span>
        <span class="sh">"</span><span class="s">of a scientific paper. </span><span class="sh">"</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Here is the list of methods </span><span class="si">{</span><span class="n">methods_string</span><span class="si">}</span><span class="s">.</span><span class="se">\n\n</span><span class="sh">"</span>
        <span class="sh">"</span><span class="s">Did the authors use any of them? </span><span class="sh">"</span>
        <span class="sh">"</span><span class="s">Answer Yes or No, followed by the name(s) of methods. </span><span class="sh">"</span>
        <span class="sh">"</span><span class="s">Use only abbreviations.</span><span class="sh">"</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">prompt</span>
</code></pre></div></div> <p>We were also able to ask questions directly about the papers using the evaluate_queries method. We utilized the RetrievalQA capability in Langchain to provide retrieval augmentation for the response generation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">evaluate_queries</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">queries</span><span class="p">):</span>
        <span class="n">chatbot</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
            <span class="n">llm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">llm</span><span class="p">,</span> 
            <span class="n">chain_type</span><span class="o">=</span><span class="sh">"</span><span class="s">stuff</span><span class="sh">"</span><span class="p">,</span> 
            <span class="n">retriever</span><span class="o">=</span><span class="n">embedding</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">similarity</span><span class="sh">"</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span><span class="mi">3</span><span class="p">})</span>
        <span class="p">)</span>
        
        <span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s"> {query}? </span><span class="sh">"""</span>
        <span class="n">response</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
                <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
                <span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">response</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">chatbot</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span>
                <span class="n">prompt</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
            <span class="p">))</span>
        <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>The results after completing the project were quite successful. Due to the nature of LLMs, we know that we need to test the accuracy to see if it is a useful tool. Using GPT 4.0, the developed system had an accuracy of .82 plus or minus 0.02 on a 95% confidence interval. Out of 20 runs, it got 17 correct, and provided two false positives and one false negative. This was more successful than GPT 3.5 which had a lower accuracy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Results-A-480.webp 480w,/assets/img/QBIHackathon/Results-A-800.webp 800w,/assets/img/QBIHackathon/Results-A-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Results-A.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The system filled out a spreadsheet with information about the papers including the methods and technologies used and provided updates through email.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Results-B-480.webp 480w,/assets/img/QBIHackathon/Results-B-800.webp 800w,/assets/img/QBIHackathon/Results-B-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Results-B.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This project was a very fun hackathon project to work on and our team ended up winning first place!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Award-480.webp 480w,/assets/img/QBIHackathon/Award-800.webp 800w,/assets/img/QBIHackathon/Award-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Award.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/QBIHackathon/Team-480.webp 480w,/assets/img/QBIHackathon/Team-800.webp 800w,/assets/img/QBIHackathon/Team-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/QBIHackathon/Team.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="key-takeaways">Key Takeaways</h2> <p>There were several key takeaways from this event.</p> <h3 id="structuring-a-project-based-on-requirements">Structuring a Project Based on Requirements</h3> <p>One of the most critical aspects of this hackathon was learning how to structure a project effectively to meet specific requirements.</p> <ol> <li>Requirement Analysis: Understanding the core problem of data scarcity in the Protein Data Bank and how to address it with automation.</li> <li>Clear Objectives: Defining clear goals such as automating the data extraction and annotation process, ensuring metadata accuracy, and facilitating integration with existing databases.</li> <li>Modular Design: Breaking down the project into manageable components like the bioRxiv scraper, document analyzer, and database management. This modular approach ensured that each part could be developed, tested, and integrated independently.</li> </ol> <h3 id="quick-delegation-and-integration-of-separate-work">Quick Delegation and Integration of Separate Work</h3> <p>Effective teamwork was another major takeaway.</p> <ol> <li>Role Assignment: Team members were assigned roles based on their expertise. For instance, some focused on backend development (like database management), others on the AI/ML components (like embeddings and LLM interaction), and others on integration and deployment.</li> <li>Regular Sync-Ups: The team held frequent check-ins to ensure everyone was on the same page, discuss progress, and troubleshoot any issues collaboratively.</li> <li>Integration Strategy: A clear integration plan was established early on. This involved setting up version control (e.g., using Git) and defining API contracts between different modules to ensure smooth integration.</li> </ol> <h3 id="working-with-llm-technologies">Working with LLM Technologies</h3> <p>The hackathon provided hands-on experience with various LLM technologies and tools.</p> <ol> <li>Vector Embeddings: Understanding how to convert text into vector embeddings using OpenAI’s embeddings and storing them in a vector database like Lantern Vector+postgres. This step was crucial for efficient retrieval and querying.</li> <li>FAISS Context Retrieval: Learning how to use FAISS for context retrieval based on cosine similarity. This involved setting up FAISS and using it to fetch relevant document fragments.</li> <li>Langchain for LLM Utilities: Utilizing Langchain to handle various LLM utilities, making the interaction with the GPT-4.0 API more seamless and efficient.</li> <li>Automating Data Annotation: Developing scripts that leverage GPT-4.0 to automate the process of annotating scientific papers, extracting relevant metadata, and updating databases.</li> </ol> <p>The repository with all of the code can be found below:</p> <div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center"> <div class="repo p-2 text-center"> <a href="https://github.com/aozalevsky/structhunt"> <img class="repo-img-light w-100" alt="aozalevsky/structhunt" src="https://github-readme-stats.vercel.app/api/pin/?username=aozalevsky&amp;repo=structhunt&amp;theme=default&amp;show_owner=true&amp;description_lines_count=1"/> <img class="repo-img-dark w-100" alt="aozalevsky/structhunt" src="https://github-readme-stats.vercel.app/api/pin/?username=aozalevsky&amp;repo=structhunt&amp;theme=dark&amp;show_owner=true&amp;description_lines_count=1"/> </a> </div> </div>]]></content><author><name></name></author><category term="Reflections"/><category term="Software"/><category term="AI"/><category term="Biotechnology"/><category term="Bioscience"/><category term="Hackathon"/><summary type="html"><![CDATA[Introduction Recently, I had the chance to join the QBI bio-hackathon at UCSF. This year’s focus was on using machine learning and artificial intelligence to advance medicine. There were many incredible projects, like breast cancer detection using imaging and protein visualization. I was excited about contributing my computing skills to medical problems. Technology advancements that improve medicine can significantly enhance people’s lives.]]></summary></entry><entry><title type="html">Crowdstrike - A Reflection</title><link href="/blog/2024/Crowdstrike/" rel="alternate" type="text/html" title="Crowdstrike - A Reflection"/><published>2024-07-31T00:00:00+00:00</published><updated>2024-07-31T00:00:00+00:00</updated><id>/blog/2024/Crowdstrike</id><content type="html" xml:base="/blog/2024/Crowdstrike/"><![CDATA[<p>In aviation, there’s a principle known as the “Swiss Cheese” model of accident prevention. This model visualizes the barriers to a severe accident as layers of Swiss cheese, with the holes representing moments when these barriers fail. This model provides the understanding the multiple systemic errors must’ve occured for a negative outcome to be attained. In such a model, we can identify what holes exist in the various layers of the cheese and make them smaller to ensure that issues are less likely.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Crowdstrike/Cheese-480.webp 480w,/assets/img/Crowdstrike/Cheese-800.webp 800w,/assets/img/Crowdstrike/Cheese-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Crowdstrike/Cheese.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When failures occur, it’s crucial to conduct an in-depth post mortem to:</p> <ul> <li>Discover the root cause of the issue</li> <li>Uncover systemic errors</li> <li>Examine future mitigation strategies</li> <li>Learn deeper lessons about complex systems and human interactions within them</li> </ul> <p>Last week, Crowdstrike, a global cybersecurity firm, pushed a faulty kernel update to their worldwide customers, crippling hospital systems, airports, and more. The estimated damage runs into billions of dollars. The root cause was traced to a single line in a configuration file that caused an unrecoverable boot loop. While many teams across the globe are now re-examining their dependencies and making changes to their systems, I want to share a few general lessons I learned from this incident as a university student.</p> <p>I am writing these lessons down to remind myself why these additional complexities are necessary. Although many of these lessons are already industry standards and experts can provide more detailed explanations, I find that articulating them helps turn abstract thoughts about good practices into concrete ideas and actionable steps.</p> <p>It is pretty obvious that Crowdstrike has made some egregious errors that could lead to an issue of this scale. Introducing automated systems and adhering the guidelines helps mitigate the potential for disaster. One of the most important aspects of any deployment pipeline is testing. Put simply, any sufficiently complex code that isn’t tested should be assumed to be wrong, really wrong, catastrophically wrong. Oftentimes, code coverage is used as a metric to determine the detail of testing. People then falsely assume that 100% code coverage guarantees no bugs. This is not the case at all. Since it is the easiest metric for determining the testing, it also falls victim to <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a>, meaning those code coverage statistics decrease in the reliability per percentage of code coverage as people attempt to optimize this statistic. This is not to say that code coverage is not important, but instead to be careful and considerate of what code coverage as a metric represents in terms of reliability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Crowdstrike/TestingTypes-480.webp 480w,/assets/img/Crowdstrike/TestingTypes-800.webp 800w,/assets/img/Crowdstrike/TestingTypes-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Crowdstrike/TestingTypes.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Thus, it is clear that testing is a very important aspect of proper bug discovery, but if code coverage isn’t a reliable metric, what is the proper way to perform testing on code? What is the industry standard? Well, with no experience in highly scalable or efficient testing systems outside of some small OSS contributions, I can’t really say. However, this <a href="https://stackify.com/unit-testing-basics-best-practices/">article</a> seems to provide some basic standards.</p> <p>When it comes to unit testing, several best practices are crucial for effective implementation. First, the “Arrange, Act, Assert” (AAA) pattern is fundamental. This involves setting up the necessary context, executing the code under test, and then verifying the results. Using relevant and high-quality test data is essential; it ensures tests are realistic and cover a variety of scenarios, including edge cases and invalid inputs. Another important practice is maintaining a one assert per test method approach to ensure clarity and pinpoint failures accurately. Avoiding test interdependence is critical, as each test should be self-contained to prevent state-related issues when tests are executed in random order. Writing tests before code, known as Test Driven Development (TDD), helps ensure code is testable, meets requirements, and is more maintainable.</p> <p>Keeping tests short, simple, and visible aids in quick understanding and maintenance. Headless testing, which runs tests without a graphical interface, can be more efficient for non-visual interactions. Testing both positive and negative scenarios ensures robustness, covering expected outcomes and error handling. Mock objects are useful for simulating dependencies and isolating the code being tested, which is vital for reliable and faster testing. Compliance with industry standards should not be overlooked, as it ensures the application meets regulatory requirements and is secure. Ensuring tests are repeatable guarantees reliability, producing the same results consistently. Finally, recognizing cumbersome test setups as a design smell can help improve code modularity and reduce brittleness. Incorporating unit tests into the build process is essential for continuous integration, ensuring that test failures halt progress and encourage immediate fixes. These practices collectively help in catching bugs early, improving code quality, enabling frequent releases, and fostering good programming habits.</p> <p>Obviously, the level of testing and methodology behind it will vary from organization to organization. NASAs testing requirements are very separate from Google’s which are very separate from a regular startup. But base testing principles are important to consider and implement for almost any software. Testing is a difficult section of development to optimize. Usually, increasing testing means greater developer time, maintenance of the testing software, and compute usage. The cost is recurring while the benefit is a prevention of a negative outcome. Any greater amount of testing, like most others things, suffers from diminishing returns. That makes it highly susceptible to cost cutting from teams attempting to reduce costs and complacency in the systems. The trade-off for testing is some calculation related to the cost of marginal testing versus the prevention of a negative outcome and the severity of that outcome on future business. These things are often difficult to quantify, like customer goodwill and brand perception. As an example, even without the lawsuits and direct customer churn, the brand damage of Crowdstrike will likely have a severe impact on their future earnings and the cost of a robust testing infrastructure would likely have increased their earnings over time. But, obviously, if they didn’t have this issue and continued to get away with poor testing practices, then there wouldn’t be the cost of testing in their business.</p> <p>While there are more testing practices that Crowdstrike should follow, every organization is different. Engineering is about tradeoffs and sometimes greater risk of failure is okay at the expense of significant testing cost. This is, of course, except when you discuss safety-critical systems. Then, there should be compliance standards that businesses follow, mandated by industry-groups and government institutions to lower the probability of negative events. Obviously, there is a lot of research on the proper way to incorporate safe engineering practices. I’m on a bit of a side-tangent, so let’s continue.</p> <p>Unit testing and integration testing are important components of a robust software testing strategy, ensuring that individual units of code work correctly and that the system functions as a cohesive whole. However, some software can benefit greatly from fuzz testing, a form of testing designed to identify vulnerabilities by inputting large amounts of random data to the system. Fuzz testing is crucial because it helps uncover edge cases and unexpected behaviors that traditional testing methods might miss. This type of testing can reveal how software handles malformed or unexpected inputs, often exposing security flaws, memory leaks, and crashes. By identifying these weaknesses, fuzz testing helps developers create more resilient and secure software, ultimately leading to a better user experience and reducing the risk of exploitation by malicious actors.</p> <p>In addition to regular testing, more formal testing may be appropriate depending on the domain. Safety-critical software often uses Formal Verification software to provide a higher level of rigor to the systems. While this level of detail likely is overkill for your standard CRUD app, barriers for using verification are decreasing making it more accessible and reasonable. Part of my research at university is using formal methods to verify operating system modeling and implementation. While much more experienced people have written extensively about this field, with all of its costs and tradeoffs, I do find it an interesting avenue for greater rigor in testing critical infrastructure systems.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Crowdstrike/FormalMethods-480.webp 480w,/assets/img/Crowdstrike/FormalMethods-800.webp 800w,/assets/img/Crowdstrike/FormalMethods-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Crowdstrike/FormalMethods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When code has passed all the testing and then the code is ready for deployment? Not quite… This is the time to test everything in action before pushing it out into the world. The staging environment is the best time to make sure everything behaves as expected in a real-world scenario without impacting customers. A staging environment is a clone of the production system with similar data.</p> <p>Another standard practice is slowly rolling out deployments and monitoring the status. This means you start rolling out the change to only a handful of customers at first and then slowly adding more and more customers until the entire deployment is completed and successful. During this process, insight into the status of customers is important. Placing status metrics and monitoring them throughout the deployment is key. Alerting systems and automatic rollback (discussed in next section) are additional tools to help in this process. With Crowdstrike being in security, often it’s imperative to roll out changes to customers quickly to protect them from cyber-threats. During critical events such as this, there should be a stronger automated process! This would help in ensuring repeated critical deployments are rolled out quickly while preventing mass-disaster as we have seen. Refining this process may take some time and practice, to find the right balance of speed and reliability, but it can be done through simulated deployments as opposed to doing this refinement process on your customers. Slow rollouts are an easy and effective strategy to reduce the damage of a prospective issue with a deployment.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Crowdstrike/DevopsMetrics-480.webp 480w,/assets/img/Crowdstrike/DevopsMetrics-800.webp 800w,/assets/img/Crowdstrike/DevopsMetrics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Crowdstrike/DevopsMetrics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When doing slow rollout, it is smart to include roll back procedures in case of failure. Rollback procedures are fairly self explanatory, but if a system is not behaving as expected during a roll out, one should quickly revert it across all customers and figure out the issue. While in the specific case of Crowdstrike this was not applicable as the update had caused computers to go into a boot loop, it is still an important lesson to reflect on in case of failure. These systems can be automated based on the metric reporting system set up in the roll out system. If some metrics pass a threshold of concern, then a roll back procedure can be initiated.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Crowdstrike/Rollback-480.webp 480w,/assets/img/Crowdstrike/Rollback-800.webp 800w,/assets/img/Crowdstrike/Rollback-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Crowdstrike/Rollback.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So to summarize the few key takeaways, proper software deployment can be complex and resource-intensive but there are systems, strategies, and tools that exist to help mitigate deployment issues. First, test the code. It’s crazy one has to say this in 2024 but untested code is crash your server code. Test it! Figure out an easy testing pipeline, figure out what is tested thoroughly and what needs work, potentially setting up quantitative and qualitative metrics to do so. In addition to regular testing, test a deployment through a staging environment and make sure no surprises arise. This ensures, if done well, a highly representative test of what the process of the deployment will be. This doesn’t only help prevent catastrophes, but can provide insight into what the update process will be like on the client side and any minor hiccups you didn’t anticipate coming to fruition. Incremental roll outs with monitoring and roll back procedures are a must. Using automated tooling to assist in this whole process reduces the manpower required and errors causing issues during the deployment process.</p> <p>But to write about important processes and systems is one thing. It’s another thing to implement it. Thus, I plan to try and implement the processes I described, at a small scale of course, with previous programs I have made.</p>]]></content><author><name></name></author><category term="Software"/><category term="Testing"/><summary type="html"><![CDATA[In aviation, there’s a principle known as the “Swiss Cheese” model of accident prevention. This model visualizes the barriers to a severe accident as layers of Swiss cheese, with the holes representing moments when these barriers fail. This model provides the understanding the multiple systemic errors must’ve occured for a negative outcome to be attained. In such a model, we can identify what holes exist in the various layers of the cheese and make them smaller to ensure that issues are less likely.]]></summary></entry><entry><title type="html">My Journey to Become a Private Pilot</title><link href="/blog/2023/Private-Pilot/" rel="alternate" type="text/html" title="My Journey to Become a Private Pilot"/><published>2023-11-03T00:00:00+00:00</published><updated>2023-11-03T00:00:00+00:00</updated><id>/blog/2023/Private-Pilot</id><content type="html" xml:base="/blog/2023/Private-Pilot/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/Sunset.JPEG-480.webp 480w,/assets/img/PrivatePilot/Sunset.JPEG-800.webp 800w,/assets/img/PrivatePilot/Sunset.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/Sunset.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Every aviator has a unique tale to tell about their journey into the world of aviation, but mine took a rather unconventional route. I started with a fear of flying, disliking the idea of being in a metal tube hurtling through the sky. However, this fear soon transformed into fascination, as I was driven to unravel the mysteries behind the incredible feat of human capability that allowed those airplanes to soar smoothly through the air. Armed with the boundless knowledge of the internet, I began a quest to understand the intricacies of the aviation system. I delved into the mechanics of flight, the intricacies of air traffic control, the rigorous training required for pilots, the complexities of meteorology, the physics of aircraft, and much more. I became entrenched in the aviation world, an obsession I would never escape. In the course of my research, I stumbled upon Microsoft Flight Simulator X, a simulation game that became my portal to aviation adventures. From landing at St. Maarten airport to embarking on F-18 missions, I was hooked. Soon, my virtual flights evolved into more sophisticated endeavors. I meticulously planned commercial routes, piloted realistic aircraft like the PMDG 737-800, and mastered standard instrument procedures. I found myself on Vatsim, an online network that united aviation enthusiasts in the virtual skies, where I polished my phraseology and technical skills. I would even make trips to SFO just to watch planes land, immersing myself in the world of controller communications via LiveATC.net.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/FrontView.JPEG-480.webp 480w,/assets/img/PrivatePilot/FrontView.JPEG-800.webp 800w,/assets/img/PrivatePilot/FrontView.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/FrontView.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I knew that I had to take my passion to the real skies. An opportunity came in the form of an introductory flight Palo Alto and south over the Santa Cruz mountains. I performed stalls, steep turns, and some pattern work. It was a defining moment, one that convinced me to pursue a pilot’s license. I threw myself into rigorous study, immersing myself in the depths of aviation systems, flight planning, meteorology, the national airspace system, and all the knowledge required to become a private pilot. My instructor, Ben, played a pivotal role in refining my knowledge for the written test. Milestones came and went during this phase, from my first solo flight, to cross-country journeys with my instructor, to the mesmerizing sights of wind turbines during a night flight to Modesto. I completed my first solo cross-country flight to Chico airport, even making a stop at Sacramento International. Finally, the culmination of my journey arrived with the checkride exam. As I successfully passed the preflight exam and executed the required maneuvers for the practical portion, I had realized my childhood dream. I had demonstrated both the knowledge and capability to achieve my goals.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/Mather.JPEG-480.webp 480w,/assets/img/PrivatePilot/Mather.JPEG-800.webp 800w,/assets/img/PrivatePilot/Mather.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/Mather.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Yet, as I reflect on my aviation journey, I am keenly aware that this is merely the beginning. The vast realm of aviation knowledge remains largely uncharted for me, and there is always more to learn. I understand the dangers of complacency, especially during the hours when a pilot starts feeling overly confident, somewhere between 200 and 500 hours of flight time. Rather than seeing this as a deterrent, I view it as a source of motivation. The idea that there is infinite room for improvement in my aviation skills excites me.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/Airport.JPEG-480.webp 480w,/assets/img/PrivatePilot/Airport.JPEG-800.webp 800w,/assets/img/PrivatePilot/Airport.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/Airport.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My aviation journey is a testament to the transformation of fear into a passionate love for the skies. It’s a story of perseverance, dedication, and an unquenchable thirst for knowledge. As I continue to soar through the boundless realm of aviation, I am reminded that there is always something new to discover, and that the joy of learning and improving is an integral part of the pilot’s journey.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/MeFlying.JPEG-480.webp 480w,/assets/img/PrivatePilot/MeFlying.JPEG-800.webp 800w,/assets/img/PrivatePilot/MeFlying.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/MeFlying.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PrivatePilot/Mountains.JPEG-480.webp 480w,/assets/img/PrivatePilot/Mountains.JPEG-800.webp 800w,/assets/img/PrivatePilot/Mountains.JPEG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/PrivatePilot/Mountains.JPEG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Flying"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Druid Heights</title><link href="/blog/2022/Druid-Heights/" rel="alternate" type="text/html" title="Druid Heights"/><published>2022-08-25T00:00:00+00:00</published><updated>2022-08-25T00:00:00+00:00</updated><id>/blog/2022/Druid-Heights</id><content type="html" xml:base="/blog/2022/Druid-Heights/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Aban1-480.webp 480w,/assets/img/DruidHeights/Aban1-800.webp 800w,/assets/img/DruidHeights/Aban1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Aban1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A few weeks ago, I had the pleasure of delving into the philosophical teachings of the celebrated writer and speaker, Alan Watts. You’ve probably encountered his inspiring talks online, as they’re widely available in the motivational speaker corner of YouTube. The specific channel I was engrossed in at the time was <a href="https://www.youtube.com/c/TrueMeaning/&quot;">TrueMeaning</a>, which is devoted entirely to sharing Alan Watts’ insightful messages.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Watts-480.webp 480w,/assets/img/DruidHeights/Watts-800.webp 800w,/assets/img/DruidHeights/Watts-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Watts.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Intrigued by the man, I promptly navigated to Wikipedia to learn more about him. To my surprise, I found that Alan Watts had passed away on Mount Tamalpais, the tallest peak in Marin County, right here in the vicinity of where I live. The specific location was a place known as Druid Heights. It soon became apparent that this was no ordinary town. According to <a href="https://en.wikipedia.org/wiki/Druid_Heights">Wikipedia</a>, Druid Heights emerged as a bohemian community with its roots in the creative spirit of the renowned poet Elsa Gidlow. It served as a gathering place for numerous influential artists and visionaries during the San Francisco counterculture era. During the 1970s, the Federal government acquired the land, either through purchase or eminent domain, and integrated it into the Golden Gate National Recreation Area. This move resulted in the abandonment and subsequent deterioration of nearly all of the buildings, with only two structures remaining.</p> <p><img src="/images/DruidHeights/Aban2.jpg" alt=""/></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Aban2-480.webp 480w,/assets/img/DruidHeights/Aban2-800.webp 800w,/assets/img/DruidHeights/Aban2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Aban2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With a newfound sense of curiosity, I decided to embark on an adventure to explore this intriguing location. After a quick call to a friend, we set off. The specific address, 732 Camino del Canyon, is somewhat secluded, and you can’t drive directly to it. Instead, you need to park in a small lot situated on Muir Woods Rd, right by the entrance to Camino Del Canyon. From there, it’s roughly a 20-minute walk along the road to reach the destination. The thick tree cover in the area made it challenging to identify individual houses, so I had to rely on a map to navigate through the region.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/DruidMap-480.webp 480w,/assets/img/DruidHeights/DruidMap-800.webp 800w,/assets/img/DruidHeights/DruidMap-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/DruidMap.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Map-480.webp 480w,/assets/img/DruidHeights/Map-800.webp 800w,/assets/img/DruidHeights/Map-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Map.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>it’s essential to note that there are a few privately owned sections within the vicinity, which are not part of the Golden Gate National Recreation Area. So, if you plan to visit, exercise caution to avoid trespassing on private property.</p> <p>As we ventured closer to the area, our attention was drawn to a collapsed building perched atop a hill. It was concealed behind thick and overgrown vegetation, making it inaccessible from our current vantage point. Undeterred, we pressed on along the path until we stumbled upon an entrance leading up the hill.</p> <p>Here are the photos we captured during our exploration of the area.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/ArtInWattLib-480.webp 480w,/assets/img/DruidHeights/ArtInWattLib-800.webp 800w,/assets/img/DruidHeights/ArtInWattLib-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/ArtInWattLib.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/ManyTrees-480.webp 480w,/assets/img/DruidHeights/ManyTrees-800.webp 800w,/assets/img/DruidHeights/ManyTrees-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/ManyTrees.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Car-480.webp 480w,/assets/img/DruidHeights/Car-800.webp 800w,/assets/img/DruidHeights/Car-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Car.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/CoolWindow-480.webp 480w,/assets/img/DruidHeights/CoolWindow-800.webp 800w,/assets/img/DruidHeights/CoolWindow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/CoolWindow.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/JapaneseMeditation-480.webp 480w,/assets/img/DruidHeights/JapaneseMeditation-800.webp 800w,/assets/img/DruidHeights/JapaneseMeditation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/JapaneseMeditation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Kitchen-480.webp 480w,/assets/img/DruidHeights/Kitchen-800.webp 800w,/assets/img/DruidHeights/Kitchen-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Kitchen.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/TahoeMap-480.webp 480w,/assets/img/DruidHeights/TahoeMap-800.webp 800w,/assets/img/DruidHeights/TahoeMap-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/TahoeMap.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DruidHeights/Window-480.webp 480w,/assets/img/DruidHeights/Window-800.webp 800w,/assets/img/DruidHeights/Window-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DruidHeights/Window.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Exploration"/><summary type="html"><![CDATA[]]></summary></entry></feed>